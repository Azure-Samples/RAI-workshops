{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# // Copyright (c) Microsoft Corporation.\n",
    "# // Licensed under the MIT license.\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moderate Content and Detect Harm with Azure AI Content Safety\n",
    "\n",
    "Contoso Camping Store is developing new AI-powered features for their website to enhance user interaction and safety. As a developer, you are tasked with integrating Azure AI Content Safety features into the store's platform. Your goal is to ensure that all customer support conversations in addition to user-generated content such as product reviews and images adhere to the company's content guidelines, promoting a safe and inclusive environment for all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Moderation\n",
    "\n",
    "Contoso Camping Store provides customers with the ability to speak with an AI-powered customer support agent and post product reviews. We could leverage an AI model to detect whether the text input from our customers is harmful and later use the detection results to implement the necessary precautions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# SAFE CONTENT\n",
    "# --------------------\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData, ImageCategory\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def analyze_text():\n",
    "    # analyze text\n",
    "    key = os.environ[\"CONTENT_SAFETY_KEY\"]\n",
    "    endpoint = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n",
    "\n",
    "    # Create an Azure AI Content Safety client\n",
    "    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "    # Construct request\n",
    "    request = AnalyzeTextOptions(text=\"Your input text\")\n",
    "\n",
    "    # Analyze text\n",
    "    try:\n",
    "        response = client.analyze_text(request)\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Analyze text failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "            raise\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    hate_result = next(item for item in response.categories_analysis if item.category == TextCategory.HATE)\n",
    "    self_harm_result = next(item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM)\n",
    "    sexual_result = next(item for item in response.categories_analysis if item.category == TextCategory.SEXUAL)\n",
    "    violence_result = next(item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE)\n",
    "\n",
    "    if hate_result:\n",
    "        print(f\"Hate severity: {hate_result.severity}\")\n",
    "    if self_harm_result:\n",
    "        print(f\"SelfHarm severity: {self_harm_result.severity}\")\n",
    "    if sexual_result:\n",
    "        print(f\"Sexual severity: {sexual_result.severity}\")\n",
    "    if violence_result:\n",
    "        print(f\"Violence severity: {violence_result.severity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# HARMFUL CONTENT\n",
    "# --------------------\n",
    "\n",
    "def analyze_text():\n",
    "    # analyze text\n",
    "    key = os.environ[\"CONTENT_SAFETY_KEY\"]\n",
    "    endpoint = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n",
    "\n",
    "    # Create an Azure AI Content Safety client\n",
    "    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "    # Construct request\n",
    "    request = AnalyzeTextOptions(text=\"Your input text\")\n",
    "\n",
    "    # Analyze text\n",
    "    try:\n",
    "        response = client.analyze_text(request)\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Analyze text failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "            raise\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    hate_result = next(item for item in response.categories_analysis if item.category == TextCategory.HATE)\n",
    "    self_harm_result = next(item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM)\n",
    "    sexual_result = next(item for item in response.categories_analysis if item.category == TextCategory.SEXUAL)\n",
    "    violence_result = next(item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE)\n",
    "\n",
    "    if hate_result:\n",
    "        print(f\"Hate severity: {hate_result.severity}\")\n",
    "    if self_harm_result:\n",
    "        print(f\"SelfHarm severity: {self_harm_result.severity}\")\n",
    "    if sexual_result:\n",
    "        print(f\"Sexual severity: {sexual_result.severity}\")\n",
    "    if violence_result:\n",
    "        print(f\"Violence severity: {violence_result.severity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# VIOLENT CONTENT WITH MISSPELLING\n",
    "# --------------------\n",
    "\n",
    "def analyze_text():\n",
    "    # analyze text\n",
    "    key = os.environ[\"CONTENT_SAFETY_KEY\"]\n",
    "    endpoint = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n",
    "\n",
    "    # Create an Azure AI Content Safety client\n",
    "    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "    # Construct request\n",
    "    request = AnalyzeTextOptions(text=\"Your input text\")\n",
    "\n",
    "    # Analyze text\n",
    "    try:\n",
    "        response = client.analyze_text(request)\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Analyze text failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "            raise\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    hate_result = next(item for item in response.categories_analysis if item.category == TextCategory.HATE)\n",
    "    self_harm_result = next(item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM)\n",
    "    sexual_result = next(item for item in response.categories_analysis if item.category == TextCategory.SEXUAL)\n",
    "    violence_result = next(item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE)\n",
    "\n",
    "    if hate_result:\n",
    "        print(f\"Hate severity: {hate_result.severity}\")\n",
    "    if self_harm_result:\n",
    "        print(f\"SelfHarm severity: {self_harm_result.severity}\")\n",
    "    if sexual_result:\n",
    "        print(f\"Sexual severity: {sexual_result.severity}\")\n",
    "    if violence_result:\n",
    "        print(f\"Violence severity: {violence_result.severity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Moderation\n",
    "\n",
    "Contoso Camping Store provides customers with the ability to upload photos to complement their product reviews. Customers have found this feature useful as it provides insight into how products look and function outside of the generic marketing images. We could leverage an AI model to detect whether the images posted by our customers are harmful and later use the detection results to implement the necessary precautions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# SAFE IMAGE\n",
    "# --------------------\n",
    "\n",
    "def analyze_image():\n",
    "    endpoint = os.environ.get('CONTENT_SAFETY_ENDPOINT')\n",
    "    key = os.environ.get('CONTENT_SAFETY_KEY')\n",
    "    image_path = os.path.join(\"sample_data\", \"image.jpg\")\n",
    "\n",
    "    # Create an Azure AI Content Safety client\n",
    "    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "\n",
    "    # Build request\n",
    "    with open(image_path, \"rb\") as file:\n",
    "        request = AnalyzeImageOptions(image=ImageData(content=file.read()))\n",
    "\n",
    "    # Analyze image\n",
    "    try:\n",
    "        response = client.analyze_image(request)\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Analyze image failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "            raise\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    hate_result = next(item for item in response.categories_analysis if item.category == ImageCategory.HATE)\n",
    "    self_harm_result = next(item for item in response.categories_analysis if item.category == ImageCategory.SELF_HARM)\n",
    "    sexual_result = next(item for item in response.categories_analysis if item.category == ImageCategory.SEXUAL)\n",
    "    violence_result = next(item for item in response.categories_analysis if item.category == ImageCategory.VIOLENCE)\n",
    "\n",
    "    if hate_result:\n",
    "        print(f\"Hate severity: {hate_result.severity}\")\n",
    "    if self_harm_result:\n",
    "        print(f\"SelfHarm severity: {self_harm_result.severity}\")\n",
    "    if sexual_result:\n",
    "        print(f\"Sexual severity: {sexual_result.severity}\")\n",
    "    if violence_result:\n",
    "        print(f\"Violence severity: {violence_result.severity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# VIOLENT IMAGE\n",
    "# --------------------\n",
    "\n",
    "def analyze_image():\n",
    "    endpoint = os.environ.get('CONTENT_SAFETY_ENDPOINT')\n",
    "    key = os.environ.get('CONTENT_SAFETY_KEY')\n",
    "    image_path = os.path.join(\"sample_data\", \"image.jpg\")\n",
    "\n",
    "    # Create an Azure AI Content Safety client\n",
    "    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "\n",
    "    # Build request\n",
    "    with open(image_path, \"rb\") as file:\n",
    "        request = AnalyzeImageOptions(image=ImageData(content=file.read()))\n",
    "\n",
    "    # Analyze image\n",
    "    try:\n",
    "        response = client.analyze_image(request)\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Analyze image failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "            raise\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    hate_result = next(item for item in response.categories_analysis if item.category == ImageCategory.HATE)\n",
    "    self_harm_result = next(item for item in response.categories_analysis if item.category == ImageCategory.SELF_HARM)\n",
    "    sexual_result = next(item for item in response.categories_analysis if item.category == ImageCategory.SEXUAL)\n",
    "    violence_result = next(item for item in response.categories_analysis if item.category == ImageCategory.VIOLENCE)\n",
    "\n",
    "    if hate_result:\n",
    "        print(f\"Hate severity: {hate_result.severity}\")\n",
    "    if self_harm_result:\n",
    "        print(f\"SelfHarm severity: {self_harm_result.severity}\")\n",
    "    if sexual_result:\n",
    "        print(f\"Sexual severity: {sexual_result.severity}\")\n",
    "    if violence_result:\n",
    "        print(f\"Violence severity: {violence_result.severity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundedness Detection\n",
    "\n",
    "Integrating an AI-powered customer support agent has been a game changer for Contoso Camping Store! Customers can ask the support agent for product recommendations and guidance on how to use Contoso Camping Store products. However, we want to ensure that the model provides responses that are grounded in the source material that’s passed onto the model.\n",
    "\n",
    "Let’s test some prompts with the model to detect the groundedness of its output.\n",
    "\n",
    "**Note**: Up to 55,000 characters of grounding sources can be analyzed in a single request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# GROUNDED Q&A\n",
    "# --------------------\n",
    "\n",
    "key = os.environ[\"CONTENT_SAFETY_KEY\"]\n",
    "endpoint = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n",
    "\n",
    "url = f'{endpoint}/contentsafety/text:detectGroundedness?api-version=2024-02-15-preview'\n",
    "subscription_key = key\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"domain\": \"Generic\",\n",
    "    \"task\": \"QnA\",\n",
    "    \"qna\": {\n",
    "        \"query\": \"<Your query>\"\n",
    "    },\n",
    "    \"text\": \"<Your text>\",\n",
    "    \"groundingSources\": [\n",
    "        \"<Your grounding source>\"\n",
    "    ],\n",
    "    \"reasoning\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# UNGROUNDED Q&A\n",
    "# --------------------\n",
    "\n",
    "url = f'{endpoint}/contentsafety/text:detectGroundedness?api-version=2024-02-15-preview'\n",
    "subscription_key = key\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"domain\": \"Generic\",\n",
    "    \"task\": \"QnA\",\n",
    "    \"qna\": {\n",
    "        \"query\": \"<Your query>\"\n",
    "    },\n",
    "    \"text\": \"<Your text>\",\n",
    "    \"groundingSources\": [\n",
    "        \"<Your grounding source>\"\n",
    "    ],\n",
    "    \"reasoning\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# SUMMARIZATION Q&A\n",
    "# --------------------\n",
    "\n",
    "url = f'{endpoint}/contentsafety/text:detectGroundedness?api-version=2024-02-15-preview'\n",
    "subscription_key = key\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"domain\": \"Generic\",\n",
    "    \"task\": \"Summarization\",\n",
    "    \"text\": \"<Your text>\",\n",
    "    \"groundingSources\": [\n",
    "        \"<Your grounding source>\"\n",
    "    ],\n",
    "    \"reasoning\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Shields\n",
    "\n",
    "Thus far, we’ve discussed ways that we could both detect harmful content and mitigate harmful content generation from the model. Let’s now add an additional layer of security to the model to prevent prompt injections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# USER PROMPT ATTACK\n",
    "# --------------------\n",
    "\n",
    "url = f'{endpoint}/contentsafety/text:shieldPrompt?api-version=2024-02-15-preview'\n",
    "subscription_key = key\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"userPrompt\": \"<Your user prompt>\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# DOCUMENT ATTACK\n",
    "# --------------------\n",
    "\n",
    "url = f'{endpoint}/contentsafety/text:shieldPrompt?api-version=2024-02-15-preview'\n",
    "subscription_key = key\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"documents\":[\"<Your documents>\"]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# PROMPT AND DOCUMENT ATTACK\n",
    "# --------------------\n",
    "\n",
    "url = f'{endpoint}/contentsafety/text:shieldPrompt?api-version=2024-02-15-preview'\n",
    "subscription_key = key\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"userPrompt\": \"<Your user prompt>\",\n",
    "    \"documents\":[\"<Your documents>\"]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
