{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Evaluation Results in Azure AI Studio\n",
    "\n",
    "Contoso Gameworks is developing an AI-powered dialogue generator for video game characters, customizing interactions based on game scenarios. The generator should be evaluated for relevance (fitting dialogue to the game's plot and character traits), fluency (smooth, immersive conversations), and risk and safety (ensuring no violent, offensive, or unfair language is introduced).\n",
    "\n",
    "In this exercise, you will run an evaluation to assess a dataset of character dialogue generated by the generator. You will push the results to your Azure AI project to track the results in Azure AI Foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add environment variables to the .env file\n",
    "\n",
    "In the root of the **Evaluation and Data Generation Workshop** folder is an `.env` file. Within the `.env` file, fill in the values for the environment variables. You can locate the values for each environment variable in the following locations of the [Azure AI Foundry](https://ai.azure.com) portal:\n",
    "\n",
    "- `AZURE_SUBSCRIPTION_ID` - On the **Overview** page of your project within **Project details**.\n",
    "- `AZURE_AI_PROJECT_NAME` - At the top of the **Overview** page for your project.\n",
    "- `AZURE_OPENAI_RESOURCE_GROUP` - On the **Overview** page of the **Management Center** within **Project properties**.\n",
    "- `AZURE_OPENAI_SERVICE` - On the **Overview** page of your project in the **Included capabilities** tab for **Azure OpenAI Service**.\n",
    "- `AZURE_OPENAI_API_VERSION` - On the [API version lifecycle](https://https://learn.microsoft.com/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release) webpage within the **Latest GA API release** section.\n",
    "- `AZURE_OPENAI_ENDPOINT` - On the **Details** tab of your model deployment within **Endpoint** (i.e. **Target URI**)\n",
    "- `AZURE_OPENAI_DEPLOYMENT_NAME` -  On the **Details** tab of your model deployment within **Deployment info**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign-in to Azure\n",
    "\n",
    "To track the evaluation results in Azure AI Studio, you'll need to first login with your Azure AI account used to provision the Azure resources.\n",
    "\n",
    "Open a new terminal and enter the following command and follow the instruction in the terminal:\n",
    "\n",
    "`az login --use-device-code`\n",
    "\n",
    "Once you've logged in, select your subscription in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve your values for assigning the Storage Blob Data Contributor role\n",
    "\n",
    "You'll need the following information to later assign yourself the **Storage Blog Data Contributor** role, which provides access to the Azure AI Project storage account. This permission is necessary for pushing the results to your Azure AI project.\n",
    "\n",
    "**Resource Group and Subscription ID**\n",
    "\n",
    "Each value can be found within the **Management center** for your project, located in [Azure AI Foundry](https://ai.azure.com). The **Management center** page is accessible via the left navigation menu of your project (at the very bottom of the navigation menu).\n",
    "\n",
    "**User ID**\n",
    "\n",
    "Enter the following command into the terminal:\n",
    "\n",
    "`az ad signed-in-user show --query id --output tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign yourself the Storage Blob Data Contributor role\n",
    "\n",
    "In the terminal, enter the following command, replacing the placeholder text with your **subscription ID**, **resource group**, and **user ID**.\n",
    "\n",
    "`az role assignment create --role \"Storage Blob Data Contributor\" --scope /subscriptions/<mySubscriptionID>/resourceGroups/<myResourceGroupName> --assignee-principal-type User --assignee-object-id \"<user-id>\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the package\n",
    "\n",
    "The evaluator classes as well as the `evaluate` function are available in the Azure AI Evaluation SDK. In addition, we'll need to use `promptflow[azure]` to track the results in our Azure AI project. We'll begin by installing all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-azure azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the environment variables.\n",
    "\n",
    "We'll import `os` and `load_dotenv` so that you can access the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We'll `json` to later dump the results into a new file. We'll also need `Path` to access the dataset. And finally, we'll need to import `evaluate` and the evaluators that we'll later use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from azure.ai.evaluation import evaluate, RelevanceEvaluator, FluencyEvaluator, ViolenceEvaluator, HateUnfairnessEvaluator, SexualEvaluator, SelfHarmEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup keyless authentication\n",
    "\n",
    "Rather than hardcode your **key**, we'll use a keyless connection with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.identity\n",
    "\n",
    "credential = azure.identity.DefaultAzureCredential()\n",
    "token_provider = azure.identity.get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "token = token_provider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model_config\n",
    "\n",
    "The `model_config` is necessary as it's a required parameter when creating an instance of an evaluator class. Let's configure the `model_config` with the following:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": token,\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Azure AI project\n",
    "\n",
    "The `azure_ai_project` is later passed into the `ContentSafetyEvaluator` to create an instance of the evaluator class. Let's configure the `azure_ai_project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"project_name\": os.environ.get(\"AZURE_AI_PROJECT_NAME\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_OPENAI_RESOURCE_GROUP\"),\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"api_key\": token,\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the path of the dataset\n",
    "\n",
    "We'll now set the path of the dataset that we'll use for the evalutation. The dataset is within the `gameworks-dialog.jsonl` file and consists of the following values for each row of data:\n",
    "\n",
    "- query\n",
    "- response\n",
    "- context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"gameworks-dialog.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of the evaluators\n",
    "\n",
    "Let's now create an instance of the **Relevance**, **Fluency** and **Content Safety** evaluators. The **Content Safety** evaluator is a composite evaluator which combines the following evaluators:\n",
    "\n",
    "- `ViolenceEvaluator`\n",
    "- `SexualEvaluator`\n",
    "- `SelfHarmEvaluator`\n",
    "- `HateUnfairnessEvaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "hateunfairness_eval = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "sexual_eval = SexualEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "selfharm_eval = SelfHarmEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the call to evaluate the dataset\n",
    "\n",
    "We can run an evaluation for a dataset with the `evaluate` function and include our list of evaluators. We must also ensure that the `evaluator_config` is set with the appropriate parameters and values for the `query`, `response`, `context` and `ground_truth`.\n",
    "\n",
    "Since we want to track the evaluation results in our Azure AI project, we'll need to include the Azure AI Foundry project information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    data=path,\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"violence\": violence_eval,\n",
    "        \"hate_unfairness\": hateunfairness_eval,\n",
    "        \"sexual\": sexual_eval,\n",
    "        \"self_harm\": selfharm_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project = azure_ai_project\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results in Azure AI Foundry\n",
    "\n",
    "Now that the evaluation is complete, you can navigate to the **Evaluation** section of the Azure AI Foundry portal to view the results. Alternatively, you can output a link to the evaluation location using the `studio_url` returned from running `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['studio_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete resources\n",
    "\n",
    "If you've finished exploring Azure AI Services, delete the Azure resource that you created during the workshop.\n",
    "\n",
    "**Note**: You may be prompted to delete your deployed model(s) before deleting the resource group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
