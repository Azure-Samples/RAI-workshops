{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Evaluation Results in Azure AI Studio\n",
    "\n",
    "Contoso Gameworks is developing an AI-powered dialogue generator for video game characters, customizing interactions based on game scenarios. The generator should be evaluated for relevance (fitting dialogue to the game's plot and character traits), fluency (smooth, immersive conversations), and risk and safety (ensuring no violent, offensive, or unfair language is introduced).\n",
    "\n",
    "In this exercise, you will run an evaluation to assess a dataset of character dialogue generated by the generator. You will push the results to your Azure AI project to track the results in Azure AI Foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign-in to Azure\n",
    "\n",
    "To track the evaluation results in Azure AI Studio, you'll need to first login with your Azure AI account used to provision the Azure resources.\n",
    "\n",
    "Open a new terminal and enter the following command and follow the instruction in the terminal:\n",
    "\n",
    "`az login --use-device-code`\n",
    "\n",
    "Once you've logged in, select your subscription in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve your values for assigning the Storage Blob Data Contributor role\n",
    "\n",
    "You'll need the following information to later assign yourself the **Storage Blog Data Contributor** role, which provides access to the Azure AI Project storage account. This permission is necessary for pushing the results to your Azure AI project.\n",
    "\n",
    "**Resource Group and Subscription ID**\n",
    "\n",
    "Each value can be found within the **Management center** for your project, located in [Azure AI Foundry](https://ai.azure.com). The **Management center** page is accessible via the left navigation menu of your project.\n",
    "\n",
    "**User ID**\n",
    "\n",
    "Enter the following command into the terminal:\n",
    "\n",
    "`az ad signed-in-user show --query id --output tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign yourself the Storage Blob Data Contributor role\n",
    "\n",
    "In the terminal, enter the following command, replacing the placeholder text with your **subscription ID**, **resource group**, and **user ID**.\n",
    "\n",
    "`az role assignment create --role \"Storage Blob Data Contributor\" --scope /subscriptions/<mySubscriptionID>/resourceGroups/<myResourceGroupName> --assignee-principal-type User --assignee-object-id \"<user-id>\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the package\n",
    "\n",
    "The evaluator classes as well as the `evaluate` function are available in the Azure AI Evaluation SDK. In addition, we'll need to use `promptflow[azure]` to track the results in our Azure AI project. We'll begin by installing all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-azure azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We'll import `os` to access our environment variables and `json` to later dump the results into a new file. We'll also need `Path` to access the dataset. And finally, we'll need to import `evaluate` and the evaluators that we'll later use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from azure.ai.evaluation import evaluate, RelevanceEvaluator, FluencyEvaluator, ViolenceEvaluator, HateUnfairnessEvaluator, SexualEvaluator, SelfHarmEvaluator\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables to create an instance of the evaluators\n",
    "\n",
    "We'll now set the environment variables that'll be required to create an instance of the evaluators. You'll need the following:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI API Key\n",
    "- Azure deployment\n",
    "\n",
    "You can locate your **Azure OpenAI endpoint** and **Azure OpenAI API Key** by navigating to **Models + endpoints**, selecting the model, and copying the respective credentials for your model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AZURE_OPENAI_ENDPOINT'] = 'Your OpenAI endpoint'\n",
    "os.environ['AZURE_OPENAI_API_KEY'] = 'Your OpenAI API key'\n",
    "os.environ['AZURE_OPENAI_DEPLOYMENT'] = 'Your Azure OpenAI API key'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model_config\n",
    "\n",
    "The `model_config` is necessary as it's a required parameter when creating an instance of an evaluator class. Let's configure the `model_config` with the following:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Azure AI project\n",
    "\n",
    "We'll now set the evironment variables that'll be required to configure the `azure_ai_project`. The `azure_ai_project` is later passed into the `ContentSafetyEvaluator` to create an instance of the evaluator class. \n",
    "\n",
    "In addition to some of the environment variables that have been configured, you'll also need the following:\n",
    "\n",
    "- Azure project name\n",
    "- Resource group name\n",
    "- Subscription ID\n",
    "\n",
    "Each value can be found within the **Management center** for your project, located in [Azure AI Foundry](https://ai.azure.com). The **Management center** page is accessible via the left navigation menu of your project.\n",
    "\n",
    "Let's first create environment variables for the variables listed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AZURE_PROJECT_NAME'] = 'Your project name'\n",
    "os.environ['RESOURCE_GROUP_NAME'] = 'Your resource group name'\n",
    "os.environ['SUBSCRIPTION_ID'] = 'Your subscription ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's configure the `azure_ai_project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "    \"resource_group_name\": os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    \"subscription_id\": os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"api_version\": os.environ.get(\"OPENAI_API_VERSION\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the path of the dataset\n",
    "\n",
    "We'll now set the path of the dataset that we'll use for the evalutation. The dataset is within the `gameworks-dialog.jsonl` file and consists of the following values for each row of data:\n",
    "\n",
    "- query\n",
    "- response\n",
    "- context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"gameworks-dialog.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of the evaluators\n",
    "\n",
    "Let's now create an instance of the **Relevance**, **Fluency** and **Content Safety** evaluators. The **Content Safety** evaluator is a composite evaluator which combines the following evaluators:\n",
    "\n",
    "- `ViolenceEvaluator`\n",
    "- `SexualEvaluator`\n",
    "- `SelfHarmEvaluator`\n",
    "- `HateUnfairnessEvaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "hateunfairness_eval = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "sexual_eval = SexualEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "selfharm_eval = SelfHarmEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the call to evaluate the dataset\n",
    "\n",
    "We can run an evaluation for a dataset with the `evaluate` function and include our list of evaluators. We must also ensure that the `evaluator_config` is set with the appropriate parameters and values for the `query`, `response`, `context` and `ground_truth`.\n",
    "\n",
    "Since we want to track the evaluation results in our Azure AI project, we'll need to include the Azure AI Foundry project information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    data=path,\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"violence\": violence_eval,\n",
    "        \"hate_unfairness\": hateunfairness_eval,\n",
    "        \"sexual\": sexual_eval,\n",
    "        \"self_harm\": selfharm_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project = azure_ai_project\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results in Azure AI Foundry\n",
    "\n",
    "Now that the evaluation is complete, you can navigate to the **Evaluation** section of the Azure AI Foundry portal to view the results. Alternatively, you can output a link to the evaluation location using the `studio_url` returned from running `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['studio_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete resources\n",
    "\n",
    "If you've finished exploring Azure AI Services, delete the Azure resource that you created during the workshop.\n",
    "\n",
    "**Note**: You may be prompted to delete your deployed model(s) before deleting the resource group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
