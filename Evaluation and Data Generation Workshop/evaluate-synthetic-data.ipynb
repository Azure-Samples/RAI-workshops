{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data and Evaluate the Dataset\n",
    "\n",
    "Contoso Zoo is developing an AI-powered smart guide app to enhance visitors' experiences. The app aims to answer questions about wildlife. To evaluate the responses from the app, we need to create a comprehensive synthetic question-answer dataset that covers various aspects of wildlife, including animal behavior, habitat preferences, and dietary needs. Once we have the synthetic dataset, we'll need to evaluate the model's responses. <br>\n",
    "\n",
    "In this exercise, you will generate a synthetic question-answer dataset for the Contoso Zoo app. This dataset will not only include standard question-answer pairs but also incorporate conversation starters to simulate contextually relevant interactions. These conversation starters will allow for pre-specified, repeatable user turns in a conversation, which is crucial for evaluating the AI's performance across different scenarios.\n",
    "\n",
    "Afterwards, we'll evaluate the model's responsese for fluency, coherence, and harm (i.e. the Risk & Safety evaluators).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "To begin, we'll install the packages that are necessary for completing the simulation. The `Simulator` class is within the `simulator` package of the Azure AI Evaluation SDK. With security in mind, we'll authenticate with Azure services using `azure-identity`. And finally, since we'll be interacting with OpenAI's services via Azure, the `openai` package will come in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation azure-identity openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We'll now import the `Simulator` class alongside several additional packages and modules that'll be necessary for running the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.ai.evaluation.simulator import Simulator\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables to create an instance of the evaluators\n",
    "\n",
    "We'll now set the environment variables that'll be required to create an instance of the evaluators. You'll need the following:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI API Key\n",
    "- Azure deployment\n",
    "- Azure OpenAI API Version\n",
    "\n",
    "You can locate your **Azure OpenAI endpoint** and **Azure OpenAI API Key** by navigating to **Deployments** and copying the respective credentials for your model deployment.\n",
    "\n",
    "The name of your Azure deployment is provided on the Deployment page.\n",
    "\n",
    "The **Azure OpenAI API** version is available within the [Latest GA API Release](https://learn.microsoft.com/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AZURE_OPENAI_ENDPOINT'] = 'Your OpenAI endpoint'\n",
    "os.environ['AZURE_OPENAI_API_KEY'] = 'Your OpenAI API key'\n",
    "os.environ['AZURE_OPENAI_DEPLOYMENT'] = 'Your deployment'\n",
    "os.environ['OPENAI_API_VERSION'] = 'The Azure OpenAI API version'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model_config\n",
    "\n",
    "The `model_config` is necessary as it's a required parameter when creating an instance of the `Simulator` class. Let's configure the `model_config` with the following:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI API key\n",
    "- Azure deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "\n",
    "should_cleanup: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of the Simulator class\n",
    "\n",
    "We now need to create an instance of the `Simulator` class which will later be used to run the simulator. We'll need to initialize the `simulator` object with the `model_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Simulator(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a call to your application\n",
    "\n",
    "We'll now create a function that'll create a call to your application which should take a user query and return a response.\n",
    "\n",
    "The call to the AI model includes parameters which specifies the model, messages, and additional settings which impact the model's response.\n",
    "\n",
    "We'll later send a query to the AI model and retrieve the repsonse. Once the repsonse is retrieved, we'll convert the completion object to a dictionary and extract the message before returning the message content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_to_your_ai_application(query: str) -> str:\n",
    "    # logic to call your application\n",
    "    # use a try except block to catch any errors\n",
    "    deployment = os.environ.get(\"AZURE_DEPLOYMENT\")\n",
    "    endpoint = os.environ.get(\"AZURE_ENDPOINT\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.environ.get(\"AZURE_API_VERSION\"),\n",
    "        api_key=os.environ.get(\"AZURE_API_KEY\"),\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    message = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    # change this to return the response from your application\n",
    "    return message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a callback\n",
    "\n",
    "We'll now create a `callback` which will handle incoming messages, process them using the app, and format the response to follow the OpenAI chat protocol format.\n",
    "\n",
    "The `callback` function will extract the latest message from the conversation history, reset the context, and then call the app to get a response based on the latest message.\n",
    "\n",
    "Afterwards, the response will be formatted to follow the OpenAI chat protocol format, and then appended to the conversation history. We'll retun the updated conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    # call your endpoint or ai application here\n",
    "    response = call_to_your_ai_application(query)\n",
    "    # we are formatting the response to follow the openAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {\n",
    "            \"citations\": None,\n",
    "        },\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create conversation starters\n",
    "\n",
    "When defining the conversation starters, we want to provide examples that are relevant to a real-world use-case. Here we provide 2 conversation starters for two separate conversations.\n",
    "\n",
    "The first conversation simulates a person discussing lions with the app. The second conversation simulates a parent inquiring about birds with the app. Both conversations are stored within a `conversations_turns` list.\n",
    "\n",
    "**Note**: Later when you run the simulator, if you consistently exceed quota, remove one of the simulations from the `conversation_turns` list and run the subsequent cells once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_turns = [\n",
    "    # simulation 1\n",
    "    [\n",
    "        \"I'm standing in front of the lion enclosure, and I've never seen a real lion before!\",  # simulator conversation starter (turn 1)\n",
    "        \"Wow, the lion is much bigger than I expected! How much do they typically weigh?\",  # conversation turn 2,\n",
    "        \"I notice the lion is just lying there. Do they sleep a lot? What do they do most of the day?\",  # conversation turn 3\n",
    "    ],\n",
    "    # simulation 2\n",
    "    [\n",
    "        \"My child is fascinated by the colorful birds in the aviary. She keeps asking about their feathers.\",\n",
    "        \"My daughter wants to know why some birds have such bright colors while others are more dull. Can you explain?\",\n",
    "        \"She's now wondering if birds can change the color of their feathers like chameleons change their skin color.\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a call to the simulator\n",
    "\n",
    "Let's now create a call to run the simulation and process the conversation turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    conversation_turns=conversation_turns,\n",
    "    max_conversation_turns=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the simulation conversation pairs into jsonl\n",
    "\n",
    "We'll now need to convert the simulated conversation into query and response pairs and place into a `.jsonl` file. A `dataset_converter` module is provided which completes the conversion using the `convert_conversations_to_jsonl()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_converter import convert_conversations_to_jsonl\n",
    "\n",
    "json_data = json.dumps(outputs)\n",
    "jsonl_output = convert_conversations_to_jsonl(json_data)\n",
    "\n",
    "# Write to a file\n",
    "with open('zoo_conversations.jsonl', 'w', encoding='utf-8') as f:\n",
    "    f.write(jsonl_output)\n",
    "\n",
    "print(\"File has been saved as 'zoo_conversations.jsonl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the path of the dataset\n",
    "\n",
    "We'll now set the path of the dataset that we'll use for the evalutation. The dataset is within the `zoo_conversations.jsonl` file and consists of the following values for each row of data:\n",
    "\n",
    "- query\n",
    "- response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"zoo_conversations.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables to create an instance of the risk and safety evaluators\n",
    "\n",
    "We'll now set the environment variables that'll be required to create an instance of each risk and safey evaluator. You'll need the following:\n",
    "\n",
    "- Azure project name\n",
    "- Resource group name\n",
    "- Subscription ID\n",
    "\n",
    "Each value can be found within [Azure AI Studio](https://ai.azure.com) by navigating to your project and viewing the **Project overview** page. If you select the link to your subscription, you'll be taken to the [Azure portal](https://portal.azure.com) which displays your subscription ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AZURE_PROJECT_NAME'] = 'Your project name'\n",
    "os.environ['RESOURCE_GROUP_NAME'] = 'Your resource group name'\n",
    "os.environ['SUBSCRIPTION_ID'] = 'Your subscription ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Azure AI project\n",
    "\n",
    "We'll now configure the Azure AI project with the following:\n",
    "\n",
    "- Azure project name\n",
    "- Resource group name\n",
    "- Subscription ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of the evaluators\n",
    "\n",
    "Let's now create an instance of the evaluators. The performance & quality evaluators (i.e. coherence and fluency) only required that we pass in the `model_config`. However, the risk and safety evaluators (i.e. violence, hate and unfairness, sexual, and self-harm) required both the `azure_ai_project` and `credential`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, CoherenceEvaluator, FluencyEvaluator, ViolenceEvaluator, HateUnfairnessEvaluator, SexualEvaluator, SelfHarmEvaluator\n",
    "from azure.identity import DefaultAzureCredential \n",
    "\n",
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "hateunfairness_eval = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "sexual_eval = SexualEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "selfharm_eval = SelfHarmEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the call to evaluate the dataset\n",
    "\n",
    "We can run an evaluation for a dataset with the `evaluate` function and include our list of evaluators. We must also ensure that the `evaluator_config` is set with the appropriate parameters and values for the `query` and `response`.\n",
    "\n",
    "Since we want to track the evaluation results in our Azure AI project, we'll need to include the Azure AI Studio project information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    data=path,\n",
    "    evaluators={\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"violence\": violence_eval,\n",
    "        \"hate_unfairness\": hateunfairness_eval,\n",
    "        \"sexual\": sexual_eval,\n",
    "        \"self_harm\": selfharm_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project = azure_ai_project\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results in Azure AI Studio\n",
    "\n",
    "Now that the evaluation is comnplete, you can navigate to the **Evaluation** section of the Azure AI Studio to view the results. Alternatively, you can output a link to the evaluation location using the `studio_url` returned from running `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['studio_url'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
