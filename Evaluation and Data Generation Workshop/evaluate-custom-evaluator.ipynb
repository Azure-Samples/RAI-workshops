{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Using a Custom Evaluator\n",
    "\n",
    "Contoso Airways is developing a customer support chatbot to assist customers with managing their flight reservations, offering real-time help with booking changes, cancellations, and seat selection. The AI-powered chatbot is designed to provide quick, accurate responses, streamlining the travel experience and reducing wait times for customer service inquiries.\n",
    "\n",
    "In this exercise, you will assess the friendliness of the customer support chatbot based on how the model responds during a customer interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add environment variables to the .env file\n",
    "\n",
    "In the root of the **Evaluation and Data Generation Workshop** folder is an `.env` file. Within the `.env` file, fill in the values for the environment variables. You can locate the values for each environment variable in the following locations of the [Azure AI Foundry](https://ai.azure.com) portal:\n",
    "\n",
    "- `AZURE_SUBSCRIPTION_ID` - On the **Overview** page of your project within **Project details**.\n",
    "- `AZURE_AI_PROJECT_NAME` - At the top of the **Overview** page for your project.\n",
    "- `AZURE_OPENAI_RESOURCE_GROUP` - On the **Overview** page of the **Management Center** within **Project properties**.\n",
    "- `AZURE_OPENAI_SERVICE` - On the **Overview** page of your project in the **Included capabilities** tab for **Azure OpenAI Service**.\n",
    "- `AZURE_OPENAI_API_VERSION` - On the [API version lifecycle](https://learn.microsoft.com/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release) webpage within the **Latest GA API release** section.\n",
    "- `AZURE_OPENAI_ENDPOINT` - On the **Details** tab of your model deployment within **Endpoint** (i.e. **Target URI**)\n",
    "- `AZURE_OPENAI_DEPLOYMENT_NAME` -  On the **Details** tab of your model deployment within **Deployment info**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the package\n",
    "\n",
    "We'll need to load the Prompty file using the `load_flow` function which is available in the `promptflow.client` package. We'll begin by installing the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the environment variables.\n",
    "\n",
    "We'll import `os` and `load_dotenv` so that you can access the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup keyless authentication\n",
    "\n",
    "Rather than hardcode your **key**, we'll use a keyless connection with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.identity\n",
    "\n",
    "credential = azure.identity.DefaultAzureCredential()\n",
    "token_provider = azure.identity.get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "token = token_provider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import promptflow\n",
    "\n",
    "We'll import `load_flow` from `promptflow` so that we can later load a flow and consume the flow object like a function by providing key-value arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import load_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model_config\n",
    "\n",
    "The `model_config` will be passed into `load_flow` when we later load the friendliness prompty. Let's configure the `model_config` with the following:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI key\n",
    "- Azure deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": token,\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create variables for the evaluation data\n",
    "\n",
    "Although we could assign the `query` and `response` strings within the call to the friendliness evaluator, we'll set them here in case you'd like to try different pairs. We've included an alternate response that could be tested by removing the `#` and commenting out the initial `response` variable. The alternate `response` provides the ability to test how the friendliness prompty scores different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I've been on hold for 30 minutes just to ask about my luggage! This is ridiculous. Where is my bag?\"\n",
    "response = \"I apologize for the long wait time, that must have been frustrating. I understand you're concerned about your luggage. Let me help you locate it right away. Could you please provide your bag tag number or flight details so I can track it for you?\"\n",
    "# response = \"Your bag is currently at the airport.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the custom evaluator and create a call to the Prompty\n",
    "\n",
    "You're now ready to load the `friendliness.prompty` file and create a call to the evaluator. Using `load_flow`, we'll load `friendliness.prompty` and pass in the `model` configuration.\n",
    "\n",
    "Next, we'll create a variable to store the score of the evaluation. To evaluate the data, we'll pass the `query` and `response` into the custom evaluator call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendliness_eval = load_flow(source=\"friendliness.prompty\", model={\"configuration\": model_config})\n",
    "friendliness_score = friendliness_eval(\n",
    "    query=query,\n",
    "    response=response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for friendliness\n",
    "\n",
    "You're now ready to run the evaluation! We'll pass the `friendliness_score` into a print statement to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(friendliness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete resources\n",
    "\n",
    "If you've finished exploring Azure AI Services, delete the Azure resource that you created during the workshop.\n",
    "\n",
    "**Note**: You may be prompted to delete your deployed model(s) before deleting the resource group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
