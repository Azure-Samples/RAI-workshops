{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Using a Custom Evaluator\n",
    "\n",
    "Contoso Airways is developing a customer support chatbot to assist customers with managing their flight reservations, offering real-time help with booking changes, cancellations, and seat selection. The AI-powered chatbot is designed to provide quick, accurate responses, streamlining the travel experience and reducing wait times for customer service inquiries.\n",
    "\n",
    "In this exercise, you will assess the emotional quotient (EQ) for the customer support chatbot based on how the model responds during a customer interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the package\n",
    "\n",
    "We'll need to load the Prompty file using the `load_flow` function which is available in the `promptflow.client` package. We'll begin by installing the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We'll import `load_flow` from `promptflow` as well as `os` so that you can access your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.client import load_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables for the model configuration\n",
    "\n",
    "We'll need to pass the model configuration to `load_flow`, which requires the following environment variables:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI API\n",
    "- Azure deployment\n",
    "- Azure OpenAI API version\n",
    "\n",
    "You can locate your **Azure OpenAI endpoint** and **Azure OpenAI API Key** by navigating to **Connections** and copying the respective credentials for your Azure OpenAI resource.\n",
    "\n",
    "The name of your **Azure deployment** is provided on the **Deployment** page.\n",
    "\n",
    "The **OpenAI API version** is available in the [API version lifecycle](https://learn.microsoft.com/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release) document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AZURE_OPENAI_ENDPOINT'] = 'Your Azure OpenAI endpoint'\n",
    "os.environ['AZURE_OPENAI_API_KEY'] = 'Your Azure OpenAI API key'\n",
    "os.environ['AZURE_DEPLOYMENT'] = 'Your Azure deployment'\n",
    "os.environ['OPENAI_API_VERSION'] = 'API version'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model_config\n",
    "\n",
    "As mentioned, the `model_config` will be passed into `load_flow` when we later load the EQ prompty. Let's configure the `model_config` with the following:\n",
    "\n",
    "- Azure OpenAI endpoint\n",
    "- Azure OpenAI key\n",
    "- Azure deployment\n",
    "- Azure OpenAI API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the EQ Prompty\n",
    "\n",
    "Let's now access the EQ evaluator which is within the `eq.prompty` file. The EQ evaluator measures the emotional quotient for a customer support chatbot. Emotional Quotient (EQ) is the ability to understand, manage, and use emotions effectively in yourself and others. \n",
    "\n",
    "You're welcome to view the file to see how the evaluator is structured!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"eq.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create variables for the evaluation data\n",
    "\n",
    "Although we could assign the `query` and `response` strings within the call to the EQ evaluator, we'll set them here in case you'd like to try different pairs. We've included an alternate response that could be tested by removing the `#` and commenting out the initial `response` variable. The alternate `response` provides the ability to test how the EQ prompty scores different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I've been on hold for 30 minutes just to ask about my luggage! This is ridiculous. Where is my bag?\"\n",
    "response = \"I apologize for the long wait time, that must have been frustrating. I understand you're concerned about your luggage. Let me help you locate it right away. Could you please provide your bag tag number or flight details so I can track it for you?\"\n",
    "# response = \"Your bag is currently at the airport.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the custom evaluator and create a call to the Prompty\n",
    "\n",
    "You're now ready to load the `eq.prompty` file and create a call to the evaluator. Using `load_flow`, we'll load `eq.prompty` and pass in the `model` configuration.\n",
    "\n",
    "Next, we'll create a variable to store the score of the evaluation. To evaluate the data, we'll pass the `query` and `response` into the custom evaluator call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_eval = load_flow(source=\"eq.prompty\", model={\"configuration\": model_config})\n",
    "eq_score = eq_eval(\n",
    "    query=query,\n",
    "    response=response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for EQ\n",
    "\n",
    "You're now ready to run the evaluation! We'll pass the `eq_score` into a print statement to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eq_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete resources\n",
    "\n",
    "If you've finished exploring Azure AI Services, delete the Azure resource that you created during the workshop.\n",
    "\n",
    "**Note**: You may be prompted to delete your deployed model(s) before deleting the resource group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
